{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "\n",
    "## CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> It is a greedy algorithm : Once a decision is made, it does not look back\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Uses either information gain or gini impurity to split the dataset\n",
    "\n",
    "1. **Gini Impurity**\n",
    "    - Probability of **incorrectly classifying a datapoint**, if that new datapoint were randomly classified according to the distribution of class labels\n",
    "    - Calculated both the branches and summed up\n",
    "    - Gini impurity is lower bounded by 0, with 0 occurring if the branch contains only one class.\n",
    "    - Upper bounded by `0.5`\n",
    "    - Total Gini = Impurity = weighted average(number of datapoint x gini impurity) of Gini Impurities for the Leaves\n",
    "    - Gini Gain is calculated for a split. ***A better split has higher Gini Gain***\n",
    "\n",
    "$$ H_{i} = \\sum_{j=1}^{c} p_{i,j} (1 - p_{i,j}) $$ \n",
    "\n",
    "where $p_{i,j}$ is the proportion of class $j$ in the $i^{th}$ branch, and $c$ is the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. **Entropy and Information Gain**\n",
    "   - Entropy is the measure of randomness in the data\n",
    "   - Information Gain is the reduction in entropy after a split\n",
    "   - a split is better if it has higher information gain, i.e. lower entropy\n",
    "\n",
    "$$ H_{i} = - \\sum_{j=1}^{c} p_{i,j} log_{2} p_{i,j} $$\n",
    "    \n",
    "    \n",
    "\n",
    "> Computationally, entropy is more complex since it makes use of **logarithms** and consequently, the calculation of the Gini-Index will be faster.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "\n",
    "- Pruning is the process of removing branches from a decision tree\n",
    "  \n",
    "Two types of pruning:\n",
    "- Pre-pruning\n",
    "    - maximum depth of the tree\n",
    "    - maximum number of leaves\n",
    "    - minimum number of samples required to split an internal node\n",
    "    - minimum impurity decrease required to split an internal node\n",
    "- Post-pruning\n",
    "    - Cost-complexity pruning\n",
    "      - a parameter $\\alpha$ is used to control the number of nodes in the tree\n",
    "      - $\\alpha$ is directly proportional to the number of pruned nodes\n",
    "      - Choose the value of $\\alpha$ using cross-validation, such that the tree generalizes well to unseen data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baggging - Bootstrap Aggregation\n",
    "\n",
    "- Sampling with replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "**Bagging + Random Subspace**\n",
    "\n",
    "- Random Subspace is randomly selecting a subset of features at each split in the tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting - Adaptive Boosting\n",
    "\n",
    "- Boosting is an ensemble method that combines a set of weak learners to form a strong learner\n",
    "  \n",
    "- AdaBoost (Adaptive Boosting)\n",
    "    - Each iteration, a new weak learner is trained on the modified version of the data\n",
    "    - The new weak learner is added to the ensemble of weak learners\n",
    "    - The weights of the training data are updated to reflect the performance of the new weak learner\n",
    "    - The process is repeated until a stopping criterion is met\n",
    "\n",
    "AdaBoost uses a weighted majority vote to make predictions  \n",
    "\n",
    "\n",
    "## Gradient Boosting - XGBoost\n",
    "\n",
    "- Gradient Boosting is a generalization of boosting to arbitrary differentiable loss functions\n",
    "- Weights of the adaboost are replaced by the negative gradient of the loss function\n",
    "- The loss function is minimized by adding weak learners to the ensemble\n",
    "- The weak learners are added sequentially, and each new learner is trained to correct the errors made by the previous learner\n",
    "- The process is repeated until a stopping criterion is met\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a62eb7397e95947974711ad7e3aa515539411275f2fd01cf7995f5d30a231c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
