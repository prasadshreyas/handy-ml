{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders\n",
    "------------\n",
    "\n",
    "Credits:\n",
    "[Jeremy Jordan's article on autoencoders](https://jeremyjordan.me/autoencoders/)\n",
    "\n",
    "The main idea behind autoencoders is to learn a representation of the data that is more compact than the original data. This is done by training the network to reconstruct the input from the output. The network is forced to learn the most important features of the data, since it has to reconstruct the input from the output. The network is trained **to minimize the reconstruction error.**\n",
    "\n",
    "Autoencoders \"encode\" the input vectors into a latent vector space, and then \"decode\" the latent vector back into the original input space. The latent vector space is a continuous space, and the autoencoder learns to map the input space to the latent space and back again.\n",
    "\n",
    "The **bottleneck** is a key attribute of our network design; without the presence of an information bottleneck, our network could easily learn to simply memorize the input values by passing these values along through the network \n",
    "\n",
    "\n",
    "\n",
    "**Types of autoencoders:**\n",
    "- Undercomplete autoencoders: The bottleneck layer \n",
    "- Sparse autoencoders: Penalize the network for having too many active units\n",
    "  \n",
    "\n",
    "**Uses:**\n",
    "- Dimensionality reduction\n",
    "- Denoising\n",
    "- Anomaly detection\n",
    "\n",
    "\n",
    "\n",
    "Variational Autoencoders\n",
    "---\n",
    "\n",
    "Instead of learning a single latent vector, the encoder learns the parameters of the distribution of all the features of the input. The decoder then samples from this distribution to generate the output. This allows the decoder to generate new samples that are similar to the training data.\n",
    "\n",
    "Our loss function for this network will consist of two terms, one which penalizes reconstruction error (which can be thought of maximizing the reconstruction likelihood as discussed earlier) and a second term which encourages our learned distribution q(z|x) to be similar to the true prior distribution p(z), which we'll assume follows a unit Gaussian distribution, for each dimension j of the latent space.\n",
    "\n",
    "$$ L = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\beta \\cdot KL(q(z|x) || p(z)) $$\n",
    "\n",
    "where KL is the Kullback-Leibler divergence, which measures the difference between two distributions. The KL divergence is a measure of how one probability distribution is different from a second, reference probability distribution. The KL divergence is non-symmetric, so KL(p||q) is not necessarily equal to KL(q||p).\n",
    "\n",
    "\n",
    "The main advantage of VAEs is that **smooth latent state representations** of the input data can be learned. \n",
    "If only KL divergence is used, the latent space will be very sparse, and the decoder will only be able to generate a small number of samples. If only the reconstruction error is used, the latent space will be very dense, and the decoder will be able to generate a large number of samples, but they will not be very similar to the training data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Reparameterization trick:\n",
    "- The reparameterization trick is a way to backpropagate through a stochastic node in a computational graph. This is useful for sampling from a distribution in a neural network, since we can't backpropagate through a sampling operation. The trick is to replace the sampling operation with a deterministic operation that is differentiable. This is done by sampling from a unit Gaussian distribution, and then scaling and shifting the result by the parameters of the distribution we want to sample from.\n",
    "  \n",
    "**Uses:**\n",
    "- Generative modeling\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers \n",
    "---\n",
    "\n",
    "A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data.\n",
    "\n",
    "\n",
    "- Attention treats each word's representation as a query to access and incorporate information from a set of values. \n",
    "\n",
    "\n",
    "\n",
    "*Resources*\n",
    "- CS224N Lecture 9 - [Link](https://youtu.be/ptuGllU5SQQ)\n",
    "- CS224N Slide 9 - [Link](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/slides/cs224n-2021-lecture09-transformers.pdf)\n",
    "\n",
    "\n",
    "> **Step by Step - Transformers**\n",
    "1. Encode position information \n",
    "    1. Compute word embeddings\n",
    "    2. Compute positional embeddings\n",
    "    3. Combine embeddings 1 and 2 into matrix $X$\n",
    "2. Take 3 linear layers and feed X into them to get key $K$, value $V$, and $Q$.\n",
    "3. Compute Attention weights $A$\n",
    "   1. Pairwise Dot Product between $Q$ and $K$ (how much does each query vector match each key vector?)\n",
    "4. $A = softmax(A)$ : Entries in the matrix, words that are related to each other, are weighted more. \n",
    "5. $ A \\dot V $ : Multiply the attention weights with the value vectors to get the **self-attention head**.\n",
    "\n",
    "The self-attention head is the building block of the Transformer. There are few barriers to using the Transformer just with self-attention.\n",
    "\n",
    "| Barries | Solution |\n",
    "| --- | --- |\n",
    "| Stacking attention layers gives weighted averages | Use FFN layers to introduce non-linearity and process the output of the attention layer|\n",
    "| In Machine Translation, ensure \"we don't look at the future\" | Use a mask to prevent the attention head from looking at the future by setting the attention weights to -infy for the padded positions |\n",
    "\n",
    "---\n",
    "> \n",
    "> Few more tricks to use the Transformer:\n",
    "> \n",
    "\n",
    "- Single vs Multi-Head Attention: \n",
    "  - Single head: \n",
    "    - Each head is a separate attention head.\n",
    "    - Each head has its own weights and biases.\n",
    "    - Each head has its own output.\n",
    "  - Multi-head: \n",
    "    - Each head is a combination of multiple attention heads.\n",
    "    - Each head has its own weights and biases.\n",
    "    - Each head has its own output.\n",
    "- Residual Connection: \n",
    "  - Instead of $$X_i = Layer(X_{i-1})$$, we use $$X_i = Layer(X_{i-1}) + X_{i-1}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a62eb7397e95947974711ad7e3aa515539411275f2fd01cf7995f5d30a231c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
